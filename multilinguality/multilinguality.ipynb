{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oT92v91OPaTe",
        "outputId": "edec15b4-2652-437f-ba92-36cbeec908e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==3.1.0a0)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2023.11.17)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.0)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16352 sha256=6abe028157ba084763a2d24de09be85a7897385066dd6c23c88f041996ff4bff\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/5d/3c/8477d0af4ca2b8b1308812c09f1930863caeebc762fe265a95\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.6\n",
            "    Uninstalling idna-3.6:\n",
            "      Successfully uninstalled idna-3.6\n",
            "Successfully installed chardet-3.0.4 googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install googletrans==3.1.0a0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBhRx0cWkEle",
        "outputId": "856f8dc8-04de-485c-eff2-25bc94b11674"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.1.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-4.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pj4Lvy0vgdMd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer\n",
        "from googletrans import Translator\n",
        "from tqdm import tqdm\n",
        "\n",
        "df = pd.read_json('dev.jsonlines', lines=True)\n",
        "\n",
        "detokenizer = TreebankWordDetokenizer()\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "translator = Translator()\n",
        "\n",
        "src_lang = \"en\"\n",
        "dest_lang = \"fr\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1cNe8A6dIv7"
      },
      "outputs": [],
      "source": [
        "def sublist_index(sub, lst):\n",
        "    sub_len = len(sub)\n",
        "    for i in range(len(lst)):\n",
        "        if lst[i:i + sub_len] == sub:\n",
        "            return (i, i + sub_len - 1)\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3MuozoSlnHa"
      },
      "outputs": [],
      "source": [
        "def get_mapping(src_start, src_end, index_mappings):\n",
        "  for mapping in index_mappings:\n",
        "    if mapping['src_start'] == src_start and mapping['src_end'] == src_end:\n",
        "      return mapping['dest_start'], mapping['dest_end']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUPMaD5JVhI-",
        "outputId": "34ff62a1-a9f3-41ac-d96b-9e70d89ee3ec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "924it [13:46,  1.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "33\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "\n",
        "trows = []\n",
        "\n",
        "for index, row in tqdm(df.iterrows()):\n",
        "  trans_row = {}\n",
        "  index_mappings = []\n",
        "  tokenized_sentence_array = []\n",
        "  trans_tok_sen_array = []\n",
        "  sentences = row.sentences\n",
        "  translated_sentences = []\n",
        "  ent_spans = row.ent_spans\n",
        "  gold_evt_links = row.gold_evt_links\n",
        "  evt_triggers = row.evt_triggers\n",
        "\n",
        "  for sentence in sentences:\n",
        "    tokenized_sentence_array = tokenized_sentence_array + sentence\n",
        "    actual_sentence = detokenizer.detokenize(sentence)\n",
        "    translated_sentence = translator.translate(actual_sentence, dest_lang, src_lang).text\n",
        "    trans_token_sen = tokenizer.tokenize(translated_sentence)\n",
        "    translated_sentences.append(trans_token_sen)\n",
        "    trans_tok_sen_array = trans_tok_sen_array + trans_token_sen\n",
        "\n",
        "  # print(translated_sentences, rel_triggers, ent_spans)\n",
        "  spans_found = True\n",
        "  trans_ent_spans = []\n",
        "  for span in ent_spans:\n",
        "    span_start = span[0]\n",
        "    span_end = span[1] + 1\n",
        "    span_type = span[2][0]\n",
        "    tokenized_arg = tokenized_sentence_array[span_start: span_end]\n",
        "    trans_tok_arg = tokenizer.tokenize(translator.translate(detokenizer.detokenize(tokenized_arg), dest_lang, src_lang).text)\n",
        "    if sublist_index(trans_tok_arg, trans_tok_sen_array) != None:\n",
        "      trans_span_start, trans_span_end = sublist_index(trans_tok_arg, trans_tok_sen_array)\n",
        "      trans_ent_spans.append([trans_span_start, trans_span_end, [span_type]])\n",
        "      translated_index_mapping = {'src_start': span_start, 'src_end': span_end-1, 'dest_start': trans_span_start, 'dest_end': trans_span_end}\n",
        "      for mapping in index_mappings:\n",
        "        if mapping['src_start'] == span_start and mapping['src_end'] == span_end-1 and mapping['dest_start'] == trans_span_start and mapping['dest_end'] == trans_span_end:\n",
        "          continue # Mapping already exists, so do not append\n",
        "      else:\n",
        "        index_mappings.append(translated_index_mapping)\n",
        "    else:\n",
        "      spans_found = False\n",
        "\n",
        "  triggers_found = True\n",
        "  trans_evt_triggers = []\n",
        "  for trigger in evt_triggers:\n",
        "    trigger_start = trigger[0]\n",
        "    trigger_end = trigger[1] + 1\n",
        "    trigger_type = trigger[2][0]\n",
        "    tok_trigger = tokenized_sentence_array[trigger_start: trigger_end]\n",
        "    trans_tok_trigger = tokenizer.tokenize(translator.translate(detokenizer.detokenize(tok_trigger), dest_lang, src_lang).text)\n",
        "    if sublist_index(trans_tok_trigger, trans_tok_sen_array) != None:\n",
        "      trans_trigger_start, trans_trigger_end = sublist_index(trans_tok_trigger, trans_tok_sen_array)\n",
        "      trans_evt_triggers.append([trans_trigger_start, trans_trigger_end, [trigger_type]])\n",
        "      translated_index_mapping = {'src_start': trigger_start, 'src_end': trigger_end-1, 'dest_start': trans_trigger_start, 'dest_end': trans_trigger_end}\n",
        "      for mapping in index_mappings:\n",
        "        if mapping['src_start'] == trigger_start and mapping['src_end'] == trigger_end-1 and mapping['dest_start'] == trans_trigger_start and mapping['dest_end'] == trans_trigger_end:\n",
        "          continue # Mapping already exists, so do not append\n",
        "      else:\n",
        "        index_mappings.append(translated_index_mapping)\n",
        "    else:\n",
        "      triggers_found = False\n",
        "\n",
        "  trans_g_evt_links = []\n",
        "  if(spans_found and triggers_found):\n",
        "    for gevt_links in gold_evt_links:\n",
        "      g_trigger_start = gevt_links[0][0]\n",
        "      g_trigger_end = gevt_links[0][1]\n",
        "      g_trans_trig_start, g_trans_trig_end = get_mapping(g_trigger_start, g_trigger_end, index_mappings)\n",
        "      g_arg_start = gevt_links[1][0]\n",
        "      g_arg_end = gevt_links[1][1]\n",
        "      g_trans_arg_start, g_trans_arg_end = get_mapping(g_arg_start, g_arg_end, index_mappings)\n",
        "      g_arg_type = gevt_links[2]\n",
        "      trans_g_evt_links.append([[g_trans_trig_start, g_trans_trig_end], [g_trans_arg_start, g_trans_arg_end], g_arg_type])\n",
        "\n",
        "    trans_row['rel_triggers'] = row.rel_triggers\n",
        "    trans_row['gold_rel_links'] = row.gold_rel_links\n",
        "    trans_row['doc_key'] = row.doc_key\n",
        "    trans_row['language_id'] = dest_lang\n",
        "    trans_row['source_url'] = row.source_url\n",
        "    trans_row['split'] = row.split\n",
        "    trans_row['sentences'] = translated_sentences\n",
        "    trans_row['ent_spans'] = trans_ent_spans\n",
        "    trans_row['evt_triggers'] = trans_evt_triggers\n",
        "    trans_row['gold_evt_links'] = trans_g_evt_links\n",
        "\n",
        "    trows.append(trans_row)\n",
        "\n",
        "\n",
        "    count = count + 1\n",
        "print()\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5i9Yvtyn4Mb"
      },
      "outputs": [],
      "source": [
        "import jsonlines\n",
        "\n",
        "filename = 'dev.fr.jsonlines'\n",
        "\n",
        "with jsonlines.open(filename, mode='w') as writer:\n",
        "    writer.write_all(trows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "w6SnRrUnc1Qj",
        "outputId": "96910a1f-8fa6-4661-ee38-3f588a2ab41b"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_268cb414-d37c-4a5a-97c1-f547f7570b40\", \"dev.ru.jsonlines\", 72444)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('dev.fr.jsonlines')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
